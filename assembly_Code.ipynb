{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d46c8-f211-43dc-ac9a-2ecf20905f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6262c7-62c8-4e34-92e8-97f1c829acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cc46fb-afc8-4f75-80a9-cb508a80851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca6d3a-7132-4f62-8f71-3e15b5a2f185",
   "metadata": {},
   "source": [
    "AIMessage(content='ciao!', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fc5d7c88-9615-48ab-a3c7-425232b562c5-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819d747-6760-4b0f-8369-9f98d853e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b732ff4-1e87-4425-a0d3-02c0530ae19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6911f-89e3-4b72-a123-83cf21207ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c07ca-8dee-478a-bd1b-32615b291cfc",
   "metadata": {},
   "source": [
    "'Ciao!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd9bd0-601e-42b8-a697-8f9c4271f599",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba900fd-dc9e-4e2a-bf94-cd7c889b37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3831e-dd1e-4217-ac0e-62209ca372fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63175e0e-dea7-461a-8296-857170759dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148597c-fe01-4f4c-9b89-099ccc58276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prompt_template.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ba5e9-6696-45ec-b72b-20d37c9c267a",
   "metadata": {},
   "source": [
    "ChatPromptValue(messages=[SystemMessage(content='Translate the following into italian:'), HumanMessage(content='hi')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c17d12-f392-4725-a962-1a8907319b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12939dcd-28e5-4a49-b084-009f9e042071",
   "metadata": {},
   "source": [
    "[SystemMessage(content='Translate the following into italian:'),\n",
    " HumanMessage(content='hi')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a0de5c-45ee-42eb-bc17-108ab56be7e3",
   "metadata": {},
   "source": [
    "## Chaining together components with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790ca3b-e112-4ad4-8a46-4d3b88add815",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253de490-6868-4275-bf5e-8cc460aa0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa476cbd-4f7c-4990-9328-d377fed14aca",
   "metadata": {},
   "source": [
    "'ciao'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758922f-ad7b-4904-9383-9ddde85d8ce0",
   "metadata": {},
   "source": [
    "## Serving with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fcb76-5c18-4cc0-877c-c25e31510bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932e2c1-b80c-4f2d-bd12-aa88aeb49e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaba9b3-a0d0-460d-a73c-948093d81567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdec7a-40f3-40d4-8a7a-a9f98225d1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e5a7a-8d51-43ac-8a17-3ffe061c338f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3ac84-5fa4-4f26-b314-70d962d7be55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39646b-d900-478c-b0aa-f981a5c3c844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5015a3-5a74-4926-a41d-56db735047a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceded7a-818f-4447-baac-14b261f324ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31ddf6-02a3-4258-bdda-a8c00fd38014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a235b-e8ca-4bd6-a8a6-2f4e540ec373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41d5e4b2-4dac-47d8-b7a3-b871b4607960",
   "metadata": {},
   "source": [
    "# ICRA paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a6541-e0db-4ef8-91d2-c8e2f61f7073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26067356-4cfb-4846-9d55-84dfc1e5509b",
   "metadata": {},
   "source": [
    "# JSON file details extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899bc4f-9aab-4911-9f85-da511e432915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AssemblyEnvironment:\n",
    "#     def __init__(self):\n",
    "#         self.base = {\n",
    "#             \"position\": {\"x\": 0, \"y\": 0, \"alpha\": 0},\n",
    "#             \"pins\": [\n",
    "#                 {\"id\": 1, \"position\": {\"x\": 10, \"y\": 20, \"alpha\": 0}},\n",
    "#                 {\"id\": 2, \"position\": {\"x\": 30, \"y\": 40, \"alpha\": 0}},\n",
    "#             ],\n",
    "#         }\n",
    "#         self.parts = [\n",
    "#             {\n",
    "#                 \"id\": \"part_1\",\n",
    "#                 \"position\": {\"x\": 100, \"y\": 200, \"alpha\": 90},\n",
    "#                 \"target_position\": {\"x\": 150, \"y\": 250, \"alpha\": 90},\n",
    "#                 \"mounting_hole\": {\"id\": 1, \"position\": {\"x\": 5, \"y\": 10, \"alpha\": 0}},\n",
    "#                 \"grip_pin\": {\"position\": {\"x\": 15, \"y\": 20, \"alpha\": 0}},\n",
    "#             },\n",
    "#             {\n",
    "#                 \"id\": \"part_2\",\n",
    "#                 \"position\": {\"x\": 300, \"y\": 400, \"alpha\": 45},\n",
    "#                 \"target_position\": {\"x\": 350, \"y\": 450, \"alpha\": 45},\n",
    "#                 \"mounting_hole\": {\"id\": 2, \"position\": {\"x\": 10, \"y\": 15, \"alpha\": 0}},\n",
    "#                 \"grip_pin\": {\"position\": {\"x\": 20, \"y\": 25, \"alpha\": 0}},\n",
    "#             },\n",
    "#         ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class AssemblyEnvironment:\n",
    "#     def __init__(self):\n",
    "#         self.base = {\n",
    "#             \"position\": {\"x\": 0, \"y\": 0, \"alpha\": 0},\n",
    "#             \"pins\": [\n",
    "#                 {\"id\": 1, \"position\": {\"x\": 10, \"y\": 20, \"alpha\": 0}},\n",
    "#                 {\"id\": 2, \"position\": {\"x\": 30, \"y\": 40, \"alpha\": 0}},\n",
    "#             ],\n",
    "#         }\n",
    "#         self.base_position = \n",
    "#         for i in range(len(self.base[\"pins\"])):\n",
    "#             pin_name = f\"base_pin_{i + 1}\"  # Create dynamic pin names like base_pin_1, base_pin_2, etc.\n",
    "#             setattr(self, pin_name, self.base[\"pins\"][i])\n",
    "        \n",
    "#         self.parts = [\n",
    "#             {\n",
    "#                 \"id\": \"part_1\",\n",
    "#                 \"position\": {\"x\": 100, \"y\": 200, \"alpha\": 90},\n",
    "#                 \"target_position\": {\"x\": 150, \"y\": 250, \"alpha\": 90},\n",
    "#                 \"mounting_hole\": {\"id\": 1, \"position\": {\"x\": 5, \"y\": 10, \"alpha\": 0}},\n",
    "#                 \"grip_pin\": {\"position\": {\"x\": 15, \"y\": 20, \"alpha\": 0}},\n",
    "#             },\n",
    "#             {\n",
    "#                 \"id\": \"part_2\",\n",
    "#                 \"position\": {\"x\": 300, \"y\": 400, \"alpha\": 45},\n",
    "#                 \"target_position\": {\"x\": 350, \"y\": 450, \"alpha\": 45},\n",
    "#                 \"mounting_hole\": {\"id\": 2, \"position\": {\"x\": 10, \"y\": 15, \"alpha\": 0}},\n",
    "#                 \"grip_pin\": {\"position\": {\"x\": 20, \"y\": 25, \"alpha\": 0}},\n",
    "#             },\n",
    "#         ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import json\n",
    "\n",
    "# class AssemblyEnvironment:\n",
    "#     def __init__(self, json_file):\n",
    "#         # Load data from the JSON file\n",
    "#         with open(json_file, 'r') as file:\n",
    "#             data = json.load(file)\n",
    "        \n",
    "#         # Initialize base and parts from the JSON data\n",
    "#         self.base = data['assembly']['base']\n",
    "#         self.parts = data['assembly']['parts']\n",
    "#         self.num_parts = len(self.parts)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create an instance of AssemblyEnvironment by providing the path to the JSON file\n",
    "#     assembly_env = AssemblyEnvironment('assembly_data.json')\n",
    "    \n",
    "#     # Print the base and parts to verify\n",
    "#     print(\"Base:\", assembly_env.base)\n",
    "#     print(\"Parts:\", assembly_env.parts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AssemblyEnvironment:\n",
    "    def __init__(self):\n",
    "        self.base = {\n",
    "            \"position\": {\"x\": 0, \"y\": 0, \"alpha\": 0},\n",
    "            \"pins\": [\n",
    "                {\"id\": 1, \"position\": {\"x\": -25, \"y\": 0, \"alpha\": 0}},\n",
    "                {\"id\": 2, \"position\": {\"x\": 25, \"y\": 0, \"alpha\": 0}},\n",
    "            ],\n",
    "        }\n",
    "        self.parts = [\n",
    "            {\n",
    "                \"id\": \"part_1\",\n",
    "                \"position\": {\"x\": -8, \"y\": -60, \"alpha\": 0},\n",
    "                \"target_position\": {\"x\": -25, \"y\": 0, \"alpha\": 0},\n",
    "                \"mounting_hole\": {\"id\": 1, \"position\": {\"x\": 0, \"y\": 0, \"alpha\": 0}},\n",
    "                \"grip_pin\": {\"position\": {\"x\": -40, \"y\": 15, \"alpha\": 0}},\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"part_2\",\n",
    "                \"position\": {\"x\": 8, \"y\": -60, \"alpha\": 0},\n",
    "                \"target_position\": {\"x\": 25, \"y\": 0, \"alpha\": 0},\n",
    "                \"mounting_hole\": {\"id\": 2, \"position\": {\"x\": 0, \"y\": 0, \"alpha\": 0}},\n",
    "                \"grip_pin\": {\"position\": {\"x\": 40, \"y\": 15, \"alpha\": 0}},\n",
    "            },\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a918e7b-cb97-4291-a9ed-22049c36bf5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe56d6d-5249-4246-a4f3-bafbd69da348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssemblyEnvironment:\n",
    "    def __init__(self, json_file):\n",
    "        with open(json_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        self.base = data['assembly']['base']\n",
    "        self.parts = data['assembly']['parts']\n",
    "        self.num_parts = len(self.parts)\n",
    "        self.base_position = data['assembly']['base']['position']\n",
    "\n",
    "        \n",
    "        # Dynamically create pin attributes\n",
    "        for i in range(len(self.base[\"pins\"])):\n",
    "            pin_name = f\"base_pin_{i + 1}\"  # Create dynamic pin names\n",
    "            setattr(self, pin_name, self.base[\"pins\"][i])\n",
    "        \n",
    "        # Extract part details dynamically\n",
    "        for i, part in enumerate(self.parts):\n",
    "            part_id = part[\"id\"]\n",
    "            setattr(self, f\"{part_id}_position\", part[\"position\"])\n",
    "            setattr(self, f\"{part_id}_target_position\", part[\"target_position\"])\n",
    "            setattr(self, f\"{part_id}_mounting_hole\", part[\"mounting_hole\"])\n",
    "            setattr(self, f\"{part_id}_grip_pin\", part[\"grip_pin\"])\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of AssemblyEnvironment by providing the path to the JSON file\n",
    "    assembly_env = AssemblyEnvironment('assembly_data.json')\n",
    "    \n",
    "    # Print the base and parts to verify\n",
    "    print(\"Base:\", assembly_env.base)\n",
    "    print(\"Parts:\", assembly_env.parts)\n",
    "    \n",
    "    # # Accessing dynamic part attributes with detailed descriptions\n",
    "    # print(\"Part 1 Position:\", assembly_env.part_1_position)  # Position of part_1\n",
    "    # print(\"Part 1 Target Position:\", assembly_env.part_1_target_position)  # Target position of part_1\n",
    "    # print(\"Part 1 Mounting Hole:\", assembly_env.part_1_mounting_hole)  # Mounting hole details of part_1\n",
    "    # print(\"Part 1 Grip Pin:\", assembly_env.part_1_grip_pin)  # Grip pin position of part_1\n",
    "    \n",
    "    # print(\"Part 2 Position:\", assembly_env.part_2_position)  # Position of part_2\n",
    "    # print(\"Part 2 Target Position:\", assembly_env.part_2_target_position)  # Target position of part_2\n",
    "    # print(\"Part 2 Mounting Hole:\", assembly_env.part_2_mounting_hole)  # Mounting hole details of part_2\n",
    "    # print(\"Part 2 Grip Pin:\", assembly_env.part_2_grip_pin)  # Grip pin position of part_2\n",
    "\n",
    "\n",
    "    # Accessing dynamic part attributes with detailed descriptions\n",
    "    for part in assembly_env.parts:\n",
    "        part_id = part[\"id\"]\n",
    "        print(f\"{part_id} Position:\", getattr(assembly_env, f\"{part_id}_position\"))  # Position of the part\n",
    "        print(f\"{part_id} Target Position:\", getattr(assembly_env, f\"{part_id}_target_position\"))  # Target position of the part\n",
    "        print(f\"{part_id} Mounting Hole:\", getattr(assembly_env, f\"{part_id}_mounting_hole\"))  # Mounting hole details of the part\n",
    "        print(f\"{part_id} Grip Pin:\", getattr(assembly_env, f\"{part_id}_grip_pin\"))  # Grip pin position of the part\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6508a-e4e6-4c44-a48f-8cf6861a0568",
   "metadata": {},
   "source": [
    "# Building agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16caa912-3c61-4eef-a484-c0dfa8227b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langgraph\n",
    "! pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a0035-1093-42ad-a63a-d241b48ef014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Now you can access your environment variables using os.environ\n",
    "os.environ['OPENAI_API_KEY'] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14ee79-e5af-4cda-a739-36e262f61d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cf074-377b-4534-9a65-79a503f4a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgentState = {}\n",
    "\n",
    "# # messages key will be assigned as an empty array. We will append new messages as we pass along nodes. \n",
    "# AgentState[\"messages\"] = []\n",
    "\n",
    "\n",
    "# def plan_generator(input_1):\n",
    "#     complete_query = \"\" + input_1\n",
    "#     response = llm.invoke(complete_query)\n",
    "#     return response.content\n",
    "\n",
    "\n",
    "# def code_generator(input_2):\n",
    "#     complete_query = \"\" + input_2\n",
    "#     response = llm.invoke(complete_query)\n",
    "#     return response.content\n",
    "\n",
    "################################################################ RAG\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "### Reading the txt files from source directory\n",
    "\n",
    "loader = DirectoryLoader('./source', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "### Creating Chunks using RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "new_docs = text_splitter.split_documents(documents=docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]\n",
    "\n",
    "###  BGE Embddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "\n",
    "### Creating Retriever using Vector DB\n",
    "\n",
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################ testing RAG\n",
    "query = \"Tell me about Japan's Industrial Growth\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(docs)\n",
    "\n",
    "\n",
    "\n",
    "def plan_generator2(state):\n",
    "    messages = state['messages']\n",
    "    question = messages[-1]   ## Fetching the user question\n",
    "    \n",
    "    complete_query = \"\" + question\n",
    "    \n",
    "    response = llm.invoke(complete_query)\n",
    "    state['messages'].append(response.content) # appending LLM call response to the AgentState\n",
    "    return state\n",
    "\n",
    "\n",
    "def code_generator2(state):\n",
    "    messages = state['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################ Supervisor\n",
    "\n",
    "\n",
    "\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"Robot1_base_holder\", \"Robot2_assembler\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a collaboration between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Use Robot1_base_holder only when the task \"\n",
    "     \"is about holding a base . Each worker will perform his\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6593028-cea8-49da-8e3b-5c55523ec957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are an AI agent which task is to provide a detail plan for fully assembling an object of {} parts. The object has a base with {} pins where {} others parts which all contains amounting hole have to be mounted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844cf25-5126-4d57-bb85-72b50fbedf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"Agent\", plan_generator2)\n",
    "workflow.add_node(\"tool\", code_generator2)\n",
    "\n",
    "workflow.add_edge('Agent', 'tool')\n",
    "\n",
    "workflow.set_entry_point(\"Agent\")\n",
    "workflow.set_finish_point(\"tool\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# inputs = {\"messages\": [\"Who came fourth for Ireland at the outdoor European Running Championships in 1998?\"]}\n",
    "# app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df52a6a-7799-4987-9b48-03d4ad7a8baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e76b9e-2e86-4b2a-b980-1d443ff8518a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc06eb3-e32e-47db-aadf-6a1d66151254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c86dd9-76dd-46e7-b324-2647d8ed96f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d730f-004f-444c-b0dd-9703ba9d612b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7393231c-b1fd-4e1d-9a49-f12549a3f227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189e19a-ff1c-4944-b30a-f8844f09908c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e04eb-8a9c-43d4-ab9c-006ce84fce4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645c800-fbbe-44bf-a356-b0ce13147bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942a989-da56-4120-a393-935d3b8be7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcacf6-9826-4ee9-b8d2-c58151e83091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034238df-2608-4ee4-ad9a-e453820ba5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351eb08-9196-41c0-a717-309cd2696394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0ed31-6ee5-406b-8141-4185ecd88319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad27c55-05af-4eef-b124-f60c0d0d0288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05bb87c-04ec-4a71-a7f2-f7d3ab2143bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4863dd-17cb-426b-9134-2ec69914f9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae967357-5d84-4f2e-aa63-b7e86789bae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c56cdec7-a694-42b6-87c2-f8ee61ac69d9",
   "metadata": {},
   "source": [
    "### Supervisor Chain creation \n",
    "\n",
    "Our team supervisor is an LLM node. It just picks the next agent to process and decides when the work is completed\n",
    "\n",
    "- Has access and information about it's memebers. \n",
    "- members = [\"RAG\" , \"Researcher\", \"Coder\"]\n",
    "- options = [\"FINISH\"] + members\n",
    "- \"Given the conversation above, who should act next?\"\n",
    "   \" Or should we FINISH? Select one of: {options}\"\n",
    "\n",
    "- Router -> function_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4100f-a70b-4621-8d13-c39b53ec43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"RAG\" , \"Researcher\", \"Coder\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Use RAG tool when questions \"\n",
    "     \"are related to Japan or of Sports category. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
